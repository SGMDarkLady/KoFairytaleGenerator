{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cebf686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\이육샛별\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ec7e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\이육샛별\\\\Franklin\\\\KoFairytaleGenerator\\\\Model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "890460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef95a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.12.21)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (45.2.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.6.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (7.6.5)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.0.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (6.7.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (21.12b0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (45.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.24)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.9.0)\n",
      "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (8.0.3)\n",
      "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.4.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.10)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens->stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: whatlies[all] in /home/danbi/.local/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: bpemb>=0.3.0 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.3.3)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (4.2.0)\n",
      "Requirement already satisfied: gensim~=3.8.3 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (3.8.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (3.5.1)\n",
      "Requirement already satisfied: tensorflow-text<2.10,>=2.9.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.2.2)\n",
      "Requirement already satisfied: spacy-lookups-data>=0.3.2; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (1.0.3)\n",
      "Requirement already satisfied: transformers>=4.19.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (4.20.1)\n",
      "Requirement already satisfied: spacy>=3.3.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (3.4.1)\n",
      "Requirement already satisfied: fasttext~=0.9.1; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.9.2)\n",
      "Requirement already satisfied: floret>=0.10.1; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.10.2)\n",
      "Requirement already satisfied: tensorflow<2.10,>=2.9.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.9.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0; extra == \"all\" in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (0.5.3)\n",
      "Requirement already satisfied: sense2vec>=2.0.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-hub<1.0,>=0.12.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.0->whatlies[all]) (4.62.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.0->whatlies[all]) (1.22.1)\n",
      "Requirement already satisfied: requests in /home/danbi/.local/lib/python3.8/site-packages (from bpemb>=0.3.0->whatlies[all]) (2.28.1)\n",
      "Requirement already satisfied: sentencepiece in /home/danbi/.local/lib/python3.8/site-packages (from bpemb>=0.3.0->whatlies[all]) (0.1.96)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (4.4.0)\n",
      "Requirement already satisfied: toolz in /home/danbi/.local/lib/python3.8/site-packages (from altair>=4.2.0->whatlies[all]) (0.12.0)\n",
      "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (1.3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (3.0.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim~=3.8.3->whatlies[all]) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim~=3.8.3->whatlies[all]) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/danbi/.local/lib/python3.8/site-packages (from gensim~=3.8.3->whatlies[all]) (6.0.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->whatlies[all]) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (9.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (4.28.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (0.11.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (1.10.1+cu111)\n",
      "Requirement already satisfied: nltk in /home/danbi/.local/lib/python3.8/site-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (3.7)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (0.11.2+cu111)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/danbi/.local/lib/python3.8/site-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (0.8.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy-lookups-data>=0.3.2; extra == \"all\"->whatlies[all]) (45.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (6.0)\n",
      "Requirement already satisfied: filelock in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (3.7.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.9.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.3.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/danbi/.local/lib/python3.8/site-packages (from fasttext~=0.9.1; extra == \"all\"->whatlies[all]) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.26.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.47.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.19.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.5.7)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.55.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2022.6.15)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (5.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18->altair>=4.2.0->whatlies[all]) (2021.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=4.2.0->whatlies[all]) (2.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/danbi/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.7.8)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.34.2)\n",
      "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.38.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (3.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip install ipywidgets\n",
    "!pip install scikit-learn\n",
    "!pip install whatlies[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0a268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  2 16:29:43 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A5000    On   | 00000000:02:00.0 Off |                  Off |\r\n",
      "| 30%   35C    P8    15W / 230W |   2723MiB / 24564MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cb629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding =\"UTF-8\") as f:\n",
    "    #with open(file_path, 'r') as f:\n",
    "        fr = f.read()\n",
    "    return fr\n",
    "\n",
    "def read_csv(file_path):\n",
    "    fr = pd.read_csv(file_path, encoding = \"utf-8\")\n",
    "    return fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db7222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"나는 알쏭달쏭 미술관 관장 '다다'란다. 이곳은 신기하고 괴상한 작품으로 가득하지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>방정환은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할아버지한테 천재문을 배웠죠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우리 할아버지입니다. 어려서부터 나는 할아버지와 한 방을 썼어요. 할아버지는 늘 신...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>핸드폰 게임 앱을 누르자 선물상자가 나왔다. 선물상자가 열리면서 수많은 옷들과 폭죽...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>그럭저럭 사이좋은 부부가 있었어요. 둘은 같이 식탁을 차렸고, 저녁을 먹은 뒤에는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>비가 개었다.  동시에 저 편 들판 건너 숲 뒤에는 둥그렇게 무지개가 뻗쳤다. 오묘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>여름 장이란 애시당초에 글러서 해는 아직 중천에 있건만 장판은 벌써 쓸쓸하고 더운 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>생물 시간이었다.  “이 없는 동물이 무엇인지 아는가?”  선생이 두 번씩 거푸 물...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>777 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    동화\n",
       "0    \"나는 알쏭달쏭 미술관 관장 '다다'란다. 이곳은 신기하고 괴상한 작품으로 가득하지...\n",
       "1    방정환은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할아버지한테 천재문을 배웠죠...\n",
       "2    텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...\n",
       "3    \"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...\n",
       "4    우리 할아버지입니다. 어려서부터 나는 할아버지와 한 방을 썼어요. 할아버지는 늘 신...\n",
       "..                                                 ...\n",
       "772  핸드폰 게임 앱을 누르자 선물상자가 나왔다. 선물상자가 열리면서 수많은 옷들과 폭죽...\n",
       "773  그럭저럭 사이좋은 부부가 있었어요. 둘은 같이 식탁을 차렸고, 저녁을 먹은 뒤에는 ...\n",
       "774  비가 개었다.  동시에 저 편 들판 건너 숲 뒤에는 둥그렇게 무지개가 뻗쳤다. 오묘...\n",
       "775  여름 장이란 애시당초에 글러서 해는 아직 중천에 있건만 장판은 벌써 쓸쓸하고 더운 ...\n",
       "776  생물 시간이었다.  “이 없는 동물이 무엇인지 아는가?”  선생이 두 번씩 거푸 물...\n",
       "\n",
       "[777 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_csv('../Dataset/dataset_0902.csv')\n",
    "dataset = dataset.drop(columns=['Unnamed: 0'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0ea627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"나는 알쏭달쏭 미술관 관장 '&lt;인물0&gt;'란다. 이곳은 신기하고 괴상한 작품으로 가...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;인물1&gt;은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할머니한테 천재문을 배웠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우리 할아버지입니다. 어려서부터 나는 할머니와 한 방을 썼어요. 할머니는 늘 신문을...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>핸드폰 게임 앱을 &lt;인물153&gt;자 선물상자가 나왔다. 선물상자가 열리면서 수많은 옷...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>그럭저럭 사이좋은 부부가 있었어요. 둘은 같이 식탁을 차렸고, 저녁을 먹은 뒤에는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>비가 개었다.  동시에 저 편 들판 건너 숲 뒤에는 둥그렇게 무지개가 뻗쳤다. 오묘...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>여름 장이란 애시당초에 글러서 해는 아직 중천에 있건만 장판은 벌써 쓸쓸하고 더운 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>생물 시간이었다.  “이 없는 동물이 무엇인지 아는가?”  선생이 두 번씩 거푸 물...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>777 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    동화\n",
       "0    \"나는 알쏭달쏭 미술관 관장 '<인물0>'란다. 이곳은 신기하고 괴상한 작품으로 가...\n",
       "1    <인물1>은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할머니한테 천재문을 배웠...\n",
       "2    텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...\n",
       "3    \"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...\n",
       "4    우리 할아버지입니다. 어려서부터 나는 할머니와 한 방을 썼어요. 할머니는 늘 신문을...\n",
       "..                                                 ...\n",
       "772  핸드폰 게임 앱을 <인물153>자 선물상자가 나왔다. 선물상자가 열리면서 수많은 옷...\n",
       "773  그럭저럭 사이좋은 부부가 있었어요. 둘은 같이 식탁을 차렸고, 저녁을 먹은 뒤에는 ...\n",
       "774  비가 개었다.  동시에 저 편 들판 건너 숲 뒤에는 둥그렇게 무지개가 뻗쳤다. 오묘...\n",
       "775  여름 장이란 애시당초에 글러서 해는 아직 중천에 있건만 장판은 벌써 쓸쓸하고 더운 ...\n",
       "776  생물 시간이었다.  “이 없는 동물이 무엇인지 아는가?”  선생이 두 번씩 거푸 물...\n",
       "\n",
       "[777 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapped_dataset = read_csv('../Dataset/swapped_data_0902.csv')\n",
    "swapped_dataset = swapped_dataset.drop(columns=['Unnamed: 0'])\n",
    "swapped_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a9ce800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"나는 알쏭달쏭 미술관 관장 \\'다다\\'란다. 이곳은 신기하고 괴상한 작품으로 가득하지. 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다. 곳곳에 커다란 물감 얼룩도 있지.\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다. 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\" \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야. 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버그는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지. 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\" \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지. \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\" \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아. 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지. 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다.\"나는 알쏭달쏭 미술관 관장 \\'<인물0>\\'란다. 이곳은 신기하고 괴상한 작품으로 가득하지. 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다. 곳곳에 커다란 물감 얼룩도 있지.\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다. 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\" \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야. 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버*그녀는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지. 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\" \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지. \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\" \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아. 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지. 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset = dataset + swapped_dataset\n",
    "concat_dataset['동화'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "914272bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(concat_dataset['동화'], test_size=0.2, shuffle=True, random_state=123)\n",
    "#trainset, testset\n",
    "\n",
    "fw = open(\"C:\\\\Users\\\\이육샛별\\\\Franklin\\\\KoFairytaleGenerator\\\\Dataset/trainset_0922.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(trainset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()\n",
    "\n",
    "fw = open(\"C:\\\\Users\\\\이육샛별\\\\Franklin\\\\KoFairytaleGenerator\\\\Dataset/testset_0922.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(testset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78c7377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "# from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", use_cache = False,\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7fceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', use_cache = False)#.to(device='cuda', non_blocking=True)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47084906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 지금부터 이야기를 시작하겠습니다. 옛날옛적에 한 오누이가 살고 있었습니다. 어느날, 하루는 그 여인이 어머니 집에 갔다가 우리집의 뜰을 봅니다. 그리고 저는 문틈으로 뭔가 비어져 나오는 것을 찾아봅니다. 마치 어머니가 그녀의 이름을 부르며 문틈으로 파고들기를 기다리는 듯했습니다. 아니, 비어져 나가는 것을 본 순간 어머니는 더 이상 그분을 알지 못합니다. 우리집에 들어오면 그 사람의 이름은 물론 어떤 모습도 보이지 않습니다. 어머니와 저는 바로 그 여인의 얼굴을 다소곳이 쳐다보기도 하면서 그녀의 그 모습을 바라보게 되었습니다. 그러자 그 여인이 \"얘, 이 여잘 낳았지?!\" 하고 물었습니다. \"오늘, 할머니는 괜찮아 보이냐?\" 아니, 그 여자는 여자의 이름을 부르며 할머니의 얼굴을 쳐다보았습니다. 여자의 얼굴이 창백해지고 아무것도 보이지 않았습니다. 여자의 얼굴은 창백해진 모습 그대로\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "지금부터 이야기를 시작하겠습니다. 옛날옛적에 한 오누이가 살고 있었습니다.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens_bfr = tokenizer.encode(prompt, return_tensors='pt')#.to(device='cuda', non_blocking=True)\n",
    "  gen_tokens_bfr = model.generate(tokens_bfr, do_sample=True, temperature=0.9, max_length=200, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              bos_token_id=tokenizer.bos_token_id)\n",
    "  generated_bfr = tokenizer.batch_decode(gen_tokens_bfr)[0].replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated_bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff293123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_trainset.txt\"\n",
    "test_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_testset.txt\"\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76905fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21c4d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0822\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=60, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    logging_steps=20,\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "    save_steps= 15000, # after # steps model is saved \n",
    "    warmup_steps=400,# number of warmup steps for learning rate scheduler\n",
    "    save_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "769cebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16893\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10342' max='15840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10342/15840 1:20:49 < 42:58, 2.13 it/s, Epoch 39.17/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>5.906689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>5.906446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>5.929402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>5.927177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>5.923293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>5.956725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>5.961024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>5.969594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>5.999148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>5.999606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>6.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>6.022871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>6.025768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>6.058149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>6.071421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>6.080831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>6.094381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>6.106766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>6.123471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>6.154196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>6.156910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>6.175467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>6.186993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>6.195515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>6.216590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>6.233203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>6.247807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>6.254884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>6.259809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>6.290055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>6.309894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>6.315757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>6.331435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>6.345001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>6.361568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>6.360255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>6.368968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>6.387169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>6.401434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>6.409692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>6.429729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>6.437250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>6.461746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>6.459434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>6.472706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>6.497109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>6.490188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>6.518878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>6.522141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>6.521810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>6.550513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>6.559628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>6.551372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>6.582603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>6.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>6.589145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>6.591374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>6.603656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>6.622678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>6.623739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>6.642147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>6.658638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>6.653213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>6.659031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>6.667568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>6.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>6.688904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>6.695778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>6.715701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>6.727272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>6.725380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.736476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>6.739078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>6.770044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>6.768599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.761606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>6.773986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>6.786847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>6.788772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>6.813533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>6.805608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>6.823657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>6.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>6.823407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.845469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>6.836195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>6.855341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.859568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>6.856849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>6.874596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>6.879575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>6.889223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>6.891972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>6.891950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>6.896856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>6.908528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>6.909407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>6.918198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.908295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>6.936914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>6.930692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>6.943425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.936104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "#optimizer.zero_grad()\n",
    "#with torch.no_grad():\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2bb500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819\n",
      "Configuration saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model weights saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bdc034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \"--\"\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path).to(device='cuda', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4c90b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 옛날 옛적에 한 오누이가 살고 있었어요.  오빠는 너무도 가난하여 양식을 구할 길이 없어 아들아이를 가지길 간절히 바랬고, 자카따룹은 세상을 떠나고 말았어요. 그때 우연히 엄마 나나가 할머니에게 말하는 것을 얻었지요. “할머니, 저하고 결혼하세요.” 사랑하는 아빠를 섬기다 가버린 애가 하루는 꿈에 삼았던 그 새가 살아 있는 걸 보았답니다. 그녀가 창가로 달려가 정말 어느 새 큰 원 안에 갇혀 있게 되었는지 궁금해졌거든요.  그러자 왕비가 이야기를 시작했습니다. “이제 떠돌아다니는 삶에도 지쳤소. 그러니 내가 하느님께 나아갈 수 있도록 도와다오.” 공주가 대답하였어.  “아니,”라며 왕이 자기와 함께 동행한 아기 곰을 데려오셨는데, 그도 마찬가지로 구혼자들이 나타나 이렇게 말하며 그가 원하는 방향으로 헤엄쳐 갔다는 거예요, 그래서 그는 마침내 그를 엄마와 아이를 살려줄 용으로 변신시키고 왕비와 나누어 달라 청하리라 부르게 된 거지 뭐죠. 하지만 이 일로 왕비는 납북절한테는 어떤 약속 말고 그냥 묻어주시기만 하면 된다는 뜻이었습니다.  넌 분명 아름답고 영리한테서 그리고 아무 것도 얻을 수가 있는데 말이 있어 그리할 것이며, 그렇다면 그렇게 말하고 싶진 않으냐 그러면서\n"
     ]
    }
   ],
   "source": [
    "#existing code\n",
    "prompt = \"\"\"\n",
    "옛날 옛적에 한 오누이가 살고 있었어요.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = finetuned_model.generate(tokens, \n",
    "                                        early_stopping=True,\n",
    "                                        do_sample=True,\n",
    "                                        temperature=0.8, \n",
    "                                        max_length=250, \n",
    "                                        repetition_penalty=3.0,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        unk_token_id=tokenizer.unk_token_id,)\n",
    "  generated = tokenizer.decode(gen_tokens[0], skip_special_tokens=True).replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
