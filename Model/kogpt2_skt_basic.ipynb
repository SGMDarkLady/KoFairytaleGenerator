{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cebf686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ec7e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/danbi/userdata/SGM_AI/darklady'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\"\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "890460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import TextDataset,DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef95a1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.12.21)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (45.2.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.6.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/danbi/.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.8/dist-packages (7.6.5)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (8.0.1)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets) (6.7.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.7)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: black in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (21.12b0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (45.2.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (3.0.24)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.8/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.16->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pathspec<1,>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.9.0)\n",
      "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (8.0.3)\n",
      "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.4.3)\n",
      "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.8/dist-packages (from black->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.8/dist-packages (from stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.10)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from asttokens->stack-data->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.14.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (3.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.7)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.22.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: whatlies[all] in /home/danbi/.local/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: bpemb>=0.3.0 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.3.3)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (4.2.0)\n",
      "Requirement already satisfied: gensim~=3.8.3 in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (3.8.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (1.1.1)\n",
      "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (3.5.1)\n",
      "Requirement already satisfied: tensorflow-text<2.10,>=2.9.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.2.2)\n",
      "Requirement already satisfied: spacy-lookups-data>=0.3.2; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (1.0.3)\n",
      "Requirement already satisfied: transformers>=4.19.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (4.20.1)\n",
      "Requirement already satisfied: spacy>=3.3.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (3.4.1)\n",
      "Requirement already satisfied: fasttext~=0.9.1; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.9.2)\n",
      "Requirement already satisfied: floret>=0.10.1; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.10.2)\n",
      "Requirement already satisfied: tensorflow<2.10,>=2.9.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.9.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0; extra == \"all\" in /usr/local/lib/python3.8/dist-packages (from whatlies[all]) (0.5.3)\n",
      "Requirement already satisfied: sense2vec>=2.0.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (2.0.0)\n",
      "Requirement already satisfied: tensorflow-hub<1.0,>=0.12.0; extra == \"all\" in /home/danbi/.local/lib/python3.8/site-packages (from whatlies[all]) (0.12.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.0->whatlies[all]) (4.62.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.0->whatlies[all]) (1.22.1)\n",
      "Requirement already satisfied: requests in /home/danbi/.local/lib/python3.8/site-packages (from bpemb>=0.3.0->whatlies[all]) (2.28.1)\n",
      "Requirement already satisfied: sentencepiece in /home/danbi/.local/lib/python3.8/site-packages (from bpemb>=0.3.0->whatlies[all]) (0.1.96)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (4.4.0)\n",
      "Requirement already satisfied: toolz in /home/danbi/.local/lib/python3.8/site-packages (from altair>=4.2.0->whatlies[all]) (0.12.0)\n",
      "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (1.3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->whatlies[all]) (3.0.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim~=3.8.3->whatlies[all]) (1.8.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim~=3.8.3->whatlies[all]) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/danbi/.local/lib/python3.8/site-packages (from gensim~=3.8.3->whatlies[all]) (6.0.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=1.0.0->whatlies[all]) (3.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (9.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (4.28.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.5.0->whatlies[all]) (0.11.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (1.10.1+cu111)\n",
      "Requirement already satisfied: nltk in /home/danbi/.local/lib/python3.8/site-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (3.7)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (0.11.2+cu111)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/danbi/.local/lib/python3.8/site-packages (from sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (0.8.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy-lookups-data>=0.3.2; extra == \"all\"->whatlies[all]) (45.2.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (6.0)\n",
      "Requirement already satisfied: filelock in /home/danbi/.local/lib/python3.8/site-packages (from transformers>=4.19.0; extra == \"all\"->whatlies[all]) (3.7.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.9.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (1.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (8.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/danbi/.local/lib/python3.8/site-packages (from spacy>=3.3.0; extra == \"all\"->whatlies[all]) (3.3.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in /home/danbi/.local/lib/python3.8/site-packages (from fasttext~=0.9.1; extra == \"all\"->whatlies[all]) (2.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.26.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.47.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.19.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.12)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.14.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (14.0.6)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/danbi/.local/lib/python3.8/site-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.1.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.5.7)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.55.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/danbi/.local/lib/python3.8/site-packages (from requests->bpemb>=0.3.0->whatlies[all]) (2022.6.15)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (5.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.18->altair>=4.2.0->whatlies[all]) (2021.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->altair>=4.2.0->whatlies[all]) (2.0.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence-transformers>=2.2.0; extra == \"all\"->whatlies[all]) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/danbi/.local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.3.0; extra == \"all\"->whatlies[all]) (0.7.8)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (2.1.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.34.2)\n",
      "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn>=0.5.0; extra == \"all\"->whatlies[all]) (0.38.1)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema>=3.0->altair>=4.2.0->whatlies[all]) (3.7.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (4.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (1.3.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0; extra == \"all\"->whatlies[all]) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb\n",
    "!pip install ipywidgets\n",
    "!pip install scikit-learn\n",
    "!pip install whatlies[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0a268f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep  2 16:29:43 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A5000    On   | 00000000:02:00.0 Off |                  Off |\r\n",
      "| 30%   35C    P8    15W / 230W |   2723MiB / 24564MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29cb629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r', encoding =\"UTF-8\") as f:\n",
    "    #with open(file_path, 'r') as f:\n",
    "        fr = f.read()\n",
    "    return fr\n",
    "\n",
    "def read_csv(file_path):\n",
    "    fr = pd.read_csv(file_path, encoding = \"utf-8\")\n",
    "    return fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5db7222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"나는 알쏭달쏭 미술관 관장 '다다'란다. 이곳은 신기하고 괴상한 작품으로 가득하지...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>방정환은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할아버지한테 천재문을 배웠죠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>우리 할아버지입니다. 어려서부터 나는 할아버지와 한 방을 썼어요. 할아버지는 늘 신...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>1421</td>\n",
       "      <td>예전 독일 나라에 프리드리히라는 대왕이 있었는데, 싸움 잘하고 지혜 많기로도 유명하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>1422</td>\n",
       "      <td>한 산 속에 암자에 덕이 높은 선생님이 있어서, 각 처에서 그 제자가 되기를 바라고...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>1423</td>\n",
       "      <td>아주 오랜 옛날 중국 사신이, 용수철같이 꾸부러진 이상스러운 구슬 한 개를 가지고,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>1424</td>\n",
       "      <td>현대 문명은 전기가 낳았다 하는 것인데 그 전기를 발명한 이는 미국 사람 프랭클린이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1425</td>\n",
       "      <td>옛날에 우리나라 유명한 양반으로 팔도어사를 지냈던 박문수란 분이 있었습니다 그분이 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                 동화\n",
       "0              0  \"나는 알쏭달쏭 미술관 관장 '다다'란다. 이곳은 신기하고 괴상한 작품으로 가득하지...\n",
       "1              1  방정환은 어려서부터 책을 참 좋아했어요. 다섯 살 때는 할아버지한테 천재문을 배웠죠...\n",
       "2              2  텔레비전도 재미있는 그림책도 없었던 옛날의 어린이들은 무엇을 하고 놀았을까요? \"옛...\n",
       "3              3  \"따뜻한 봄바람이 살랑살랑 불고 겨울에 움츠렸던 몸이 움찔움찔. 얘들아 나랑 놀자 ...\n",
       "4              4  우리 할아버지입니다. 어려서부터 나는 할아버지와 한 방을 썼어요. 할아버지는 늘 신...\n",
       "...          ...                                                ...\n",
       "1096        1421  예전 독일 나라에 프리드리히라는 대왕이 있었는데, 싸움 잘하고 지혜 많기로도 유명하...\n",
       "1097        1422  한 산 속에 암자에 덕이 높은 선생님이 있어서, 각 처에서 그 제자가 되기를 바라고...\n",
       "1098        1423  아주 오랜 옛날 중국 사신이, 용수철같이 꾸부러진 이상스러운 구슬 한 개를 가지고,...\n",
       "1099        1424  현대 문명은 전기가 낳았다 하는 것인데 그 전기를 발명한 이는 미국 사람 프랭클린이...\n",
       "1100        1425  옛날에 우리나라 유명한 양반으로 팔도어사를 지냈던 박문수란 분이 있었습니다 그분이 ...\n",
       "\n",
       "[1101 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = read_csv('/home/danbi/userdata/SGM_AI/darklady/dataset/dataset_0831.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c0ea627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>동화</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022년 5월 5일은 &lt;인물1&gt; 선생님이 어린이날을 만든 지 100주년이 되는 날...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>1309</td>\n",
       "      <td>옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>1310</td>\n",
       "      <td>어느 커다란 도시에 나이 드신 어머님 한 분이 자여신의 방에 저녁 내내 앉아 그동안...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1311</td>\n",
       "      <td>가난한 시골 소녀가 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1312</td>\n",
       "      <td>어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 *그녀의 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>1313</td>\n",
       "      <td>하루는 노인과 *그녀의 남편이 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                                 동화\n",
       "0              0   우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 ...\n",
       "1              1  2022년 5월 5일은 <인물1> 선생님이 어린이날을 만든 지 100주년이 되는 날...\n",
       "2              2  5월은 가정의 달이라고 할 만큼 가족들의 소중함을 생각해보게 하는 기념일이 참 많은...\n",
       "3              3   우리 친구들은 또래 친구들을 만나면 보통 어떤 놀이를 하면서 시간을 보내나요?  ...\n",
       "4              4  6월 6일은 나라를 지키기 위해 목숨을 바친 모든 이들의 넋을 기리는 법정 기념일 ...\n",
       "...          ...                                                ...\n",
       "1309        1309  옛날에 한 번은 마부가 포도주를 잔득 실은 ‘달구지’를 끌고 가다 그만 속도를 너무...\n",
       "1310        1310  어느 커다란 도시에 나이 드신 어머님 한 분이 자여신의 방에 저녁 내내 앉아 그동안...\n",
       "1311        1311  가난한 시골 소녀가 어느 날 교회에서 목사님이 “하늘나라 왕국에 들어가길 원하는 자...\n",
       "1312        1312  어느 날 오후에 아기 예수가 아기침대에 누워 잠을 자고 있었답니다. 그때 *그녀의 ...\n",
       "1313        1313  하루는 노인과 *그녀의 남편이 누추한 집 앞에 앉아 잠시 일손을 놓고 휴식들을 취하...\n",
       "\n",
       "[1314 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swapped_dataset = read_csv('/home/danbi/userdata/SGM_AI/darklady/dataset/swapped_data_0822.csv')\n",
    "swapped_dataset#['동화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a9ce800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"나는 알쏭달쏭 미술관 관장 \\'다다\\'란다. 이곳은 신기하고 괴상한 작품으로 가득하지. 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다. 곳곳에 커다란 물감 얼룩도 있지.\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다. 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\" \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야. 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버그는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지. 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\" \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지. \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\" \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아. 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지. 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다. 우리 친구들은 미술이 뭐라고 생각하나요.  종이에 그림을 그리는 것 아니면 멋진 조각 작품을 만드는 것 그런데 지금의 미술은 꼭 그림이나 조각품만이 아니랍니다.  비디오 아트 행위 예술 벽의 낙서까지 그 어떤 것도 예술이고 미술일 수 있다고 해요. 그럼 오늘 신나는 동화여행 속에서 알쏭달쏭 현대미술관으로 재미있고 엉뚱한 작품을 만나러 가볼까요.  알쏭달쏭 미술관 \"나는 알쏭달쏭 미술관 관장 \\'<인물0>\\'란다 이곳은 신기하고 괴상한 작품으로 가득하지 재미있는 미술 작품들이 가득한 알쏭달쏭 미술관에 온 걸 환영합니다.\" \"이쪽에 있는 변기를 볼래? 화장실에서나 볼 수 있는 변기가 미술 작품이라면 믿겠니?\" \"네? 변기가 미술 작품이라구요?\" \"너희들 모나리자란 그림을 알고 있지? 레오나르도 다빈치가 그린 그림이야. 그런데 뒤샹이라는 화가는 모나리자의 얼굴에 수염만 그리고 전시를 했단다. 그래서 사람들은 깜짝 놀랐지.\" \"네? 유명한 작품에 낙서를 한 게 미술 작품이라구요?\" \"뒤샹은 장난꾸러기였나봐. 그럼 이거는 뭘 그린 건지 한번 맞춰볼래? 여러 색깔의 물감을 뿌리고 붓고 흘린 거란다 곳곳에 커다란 물감 얼룩도 있지\" \"폴록이라는 화가는 이게 연보라빛 안개라는구나. 그런데 미술관 관장인 내가 봐도 신나게 물감을 뿌려 놓은 것 같거든 혹시 너희들 눈에는 안개가 보이니?\" \"글쎄요? 보이는 것도 같고 아닌 것 같기도 하고..\" \"그렇게 볼록은 커다란 종류의 물감을 쏟아 부었어. 그리고 붓을 휘젓고 물감을 튀겼지. 가끔은 물감 대신 모래를 톡톡 뿌리기도 했단다. 진짜 신났겠지?\" \"물감을 가지고 장난치는 것 같아...\" \"화가 에른스트는 물건의 종이를 대고 문지르면서 작품을 만들었어. 어떤 물건이냐에 따라 무늬가 달라지지!\" \"에이 이건 나도 그릴 수 있을 것 같아.\" \"종이에 물감을 쭉 짜놓고 접었다가 펴면 양쪽에 똑같은 그림이 나타나지 이걸 \\'데칼코마니\\'라고 한단다.\" \"이 그림은 어떠냐 이것도 미술 작품이야. 리엔텐 슈타인은 만화의 한 장면에 검은 선을 그린 다음 알록달록 색칠을 했단다 진짜 만화책처럼 보이려고 촘촘히 작은 점도 찍었어.\"  \"관장님 저도 만화책으로 작품을 만들어 볼래요!\" \"그리고 리히텐슈타인은 말풍선 안에 글도 넣었단다. 누구나 알기 쉽게 재미있는 그림을 그리고 싶었던 거야 어때 만화를 보는 것처럼 쉽지?\" \"네 꼭 만화책 같아요.\" \"햄버거와 막대 아이스크림을 미술 작품이라고 전시한 미술가도 있단다. 먹고 싶다기보다는 깔고 앉고 싶지.\" \"물론 절대로 먹을 수는 없어 고무와 상자 천으로 만들어졌거든.\" \"야 이런 것도 현대미술이구나. 어 갑자기 배가 고파져.\" \"화가 올덴버*그녀는 반대로 생각하는 걸 좋아했단다. 사람이나 눈물 대신 물건이나 음식을 조각으로 만들었어. \"숟가락, 미끄럼틀, 먹다 버린 사과, 배드민턴 공 이런 재료들이 모두 미술이 되었지 재료도 이것저것 써서 작은 곳은 엄청 크게 단단한 곳은 물렁물렁하게 만들었단다.\" \"먹다 버린 사과가 멋진 작품이 된 거야.\"  \"심심해서 끄적거리는 것도 미술이 될까?\" \"화가 바스키아는 지하철과 길거리 벽에 글을 쓰고 그림을 그렸어. 아프리카 가면을 닮은 얼굴이랑 글자 화려한 색깔과 모양이 어우러져 활기찬 그림이 되었지.\" \"낙서가 예술 작품이 될 수 있다구요? 어 그럼 나도 벌써 예술간데..?\" \"스미스슨이라는 화가는 호수의 빙글빙글 달팽이 집 모양 둑을 만들었어. 이건 비행기를 타야 볼 수 있겠지.  \"저기요 저기 멋진 둑이 보여요.\" \"이 둑은 물에 잠기기도 하고 부서지거나 깎이면서 조금씩 없어진단다. 이런 것까지도 작품이라고 생각했단다. 현대 미술가들은 톡톡 튀는 생각 톡톡 튀는 방법으로 작품을 만들어요.\" \"허스트는 죽은 동물이나 알약으로 죽음을 표현했단다.\" \"그런데 이게 모두 미술이 맞는 걸까?\"  \"미술이 그림을 그리는 것만은 아닌가 봐요. 그래 맞아 현대 미술가들은 자기가 그리고 싶은 대로 그리고 만들고 싶은 대로 만들지 우리는 그냥 보이는 대로 느끼면 된단다.\" 정말 생각지도 않았던 것들이 미술 작품으로 전시되어 있었어요. 이렇게 현대미술은 표현 방법이나 주제 또 재료와 도구들이 아주 아주 다양해졌답니다. 우리 친구들도 느끼는 대로 생각하는 대로 머릿속에 그려지는 상상력을 마음껏 나타내 보세요. 바로 그게 미술이니까요. 그럼 우리는 다음에 또 다른 이야기로 다시 만나요. 안녕~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_dataset = dataset + swapped_dataset\n",
    "concat_dataset['동화'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "914272bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(concat_dataset['동화'], test_size=0.2, shuffle=True, random_state=123)\n",
    "#trainset, testset\n",
    "\n",
    "fw = open(\"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_trainset.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(trainset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()\n",
    "\n",
    "fw = open(\"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_testset.txt\", \"w\", encoding = \"utf-8\")\n",
    "fw.write(testset.str.cat(sep=' ').replace('\\n', ' '))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78c7377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import PreTrainedTokenizerFast\n",
    "# from transformers import GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", use_cache = False,\n",
    "                                                    bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "                                                    pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7fceb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', use_cache = False).to(device='cuda', non_blocking=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47084906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 지금부터 이야기를 시작하겠습니다. 옛날옛적에 한 오누이가 살고 있었습니다. \"그거 보시오!\" 아이의 아버지는 \"아씨! 아씨!\"하며 말했습니다. 그러나 오누이는 \"어찌 이놈아!\" 하며 웃고 말았습니다. \"아씨! 아씨를 봤지?\" 아이는 잠시 생각해 보더니 그제야 그 이야기를 할 수 있었습니다. 하지만 아씨는 <unk><unk>거리며 말했습니다. \"저기 한 여자가 한 방에 있었소.\" 이러한 이야기를 들은 오누이는 집으로 돌아왔습니다. 오누이 아버지의 집은 집주인의 집이었습니다. 또한 한 명이 집 주인이 되어 집을 지키고 있었습니다. \"한 여자, 바로 우리네 동네 그 동네 사람이예.\" 그러자 한 여자는 대답을 하였습니다. \"그게 아니라 그냥 바로 그 여자에게 가서 말했잖아요. 아씨도 그랬을 거예요.\" 아씨는 아주 조금 의아하였습니다. 여자에게서 말을 듣지 않았으니 말입니다. 만약\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "지금부터 이야기를 시작하겠습니다. 옛날옛적에 한 오누이가 살고 있었습니다.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens_bfr = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens_bfr = model.generate(tokens_bfr, do_sample=True, temperature=0.9, max_length=200, \n",
    "                              pad_token_id=tokenizer.pad_token_id,\n",
    "                              eos_token_id=tokenizer.eos_token_id,\n",
    "                              bos_token_id=tokenizer.bos_token_id)\n",
    "  generated_bfr = tokenizer.batch_decode(gen_tokens_bfr)[0].replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated_bfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff293123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "     \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator\n",
    "\n",
    "train_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_trainset.txt\"\n",
    "test_path = \"/home/danbi/userdata/SGM_AI/darklady/dataset/tmp_testset.txt\"\n",
    "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76905fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "21c4d31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments,AutoModelWithLMHead\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0822\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=60, # number of training epochs\n",
    "    per_device_train_batch_size=64, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    logging_steps=20,\n",
    "    eval_steps = 100, # Number of update steps between two evaluations.\n",
    "    save_steps= 15000, # after # steps model is saved \n",
    "    warmup_steps=400,# number of warmup steps for learning rate scheduler\n",
    "    save_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    dataloader_pin_memory=False,\n",
    "    report_to=\"none\",\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "769cebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 16893\n",
      "  Num Epochs = 60\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15840\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10342' max='15840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10342/15840 1:20:49 < 42:58, 2.13 it/s, Epoch 39.17/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>5.906689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.230400</td>\n",
       "      <td>5.906446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.243200</td>\n",
       "      <td>5.929402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.281500</td>\n",
       "      <td>5.927177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>5.923293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.277300</td>\n",
       "      <td>5.956725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>5.961024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>5.969594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>5.999148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>5.999606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>6.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261600</td>\n",
       "      <td>6.022871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.273400</td>\n",
       "      <td>6.025768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>6.058149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>6.071421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.227700</td>\n",
       "      <td>6.080831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>6.094381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.246100</td>\n",
       "      <td>6.106766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.218400</td>\n",
       "      <td>6.123471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>6.154196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>6.156910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.206300</td>\n",
       "      <td>6.175467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>6.186993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.191400</td>\n",
       "      <td>6.195515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>6.216590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>6.233203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>6.247807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>6.254884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.202100</td>\n",
       "      <td>6.259809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.183400</td>\n",
       "      <td>6.290055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>6.309894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.169000</td>\n",
       "      <td>6.315757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>6.331435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.182700</td>\n",
       "      <td>6.345001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.164100</td>\n",
       "      <td>6.361568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.173500</td>\n",
       "      <td>6.360255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.170900</td>\n",
       "      <td>6.368968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>6.387169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>6.401434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>6.409692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>6.429729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.160800</td>\n",
       "      <td>6.437250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.146100</td>\n",
       "      <td>6.461746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>6.459434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>6.472706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.144800</td>\n",
       "      <td>6.497109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.146600</td>\n",
       "      <td>6.490188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.132500</td>\n",
       "      <td>6.518878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>6.522141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.143400</td>\n",
       "      <td>6.521810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.133600</td>\n",
       "      <td>6.550513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>6.559628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>6.551372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>6.582603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>6.576531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.121900</td>\n",
       "      <td>6.589145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>6.591374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>6.603656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>6.622678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>6.623739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>6.642147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>6.658638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.118700</td>\n",
       "      <td>6.653213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>6.659031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>6.667568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.117400</td>\n",
       "      <td>6.669468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>6.688904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>6.695778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>6.715701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.106300</td>\n",
       "      <td>6.727272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>6.725380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.736476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>6.739078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>6.770044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>6.768599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>6.761606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.095000</td>\n",
       "      <td>6.773986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.097300</td>\n",
       "      <td>6.786847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.099600</td>\n",
       "      <td>6.788772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.092100</td>\n",
       "      <td>6.813533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.096300</td>\n",
       "      <td>6.805608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.089700</td>\n",
       "      <td>6.823657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>6.828061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>6.823407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.845469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.091300</td>\n",
       "      <td>6.836195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.090900</td>\n",
       "      <td>6.855341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>6.859568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.087600</td>\n",
       "      <td>6.856849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>6.874596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>6.879575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>6.889223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>6.891972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.082900</td>\n",
       "      <td>6.891950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>6.896856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>6.908528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.083700</td>\n",
       "      <td>6.909407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.076500</td>\n",
       "      <td>6.918198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.908295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>6.936914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>6.930692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>6.943425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>6.936104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3557\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "#optimizer.zero_grad()\n",
    "#with torch.no_grad():\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b2bb500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819\n",
      "Configuration saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model weights saved in /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9bdc034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"created_date\": \"2021-04-28\",\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"license\": \"CC-BY-NC-SA 4.0\",\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "loading weights file /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /home/danbi/userdata/SGM_AI/darklady/kogpt2_finetuned_0819.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model_path = \"--\"\n",
    "finetuned_model = GPT2LMHeadModel.from_pretrained(finetuned_model_path).to(device='cuda', non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d4c90b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 옛날 옛적에 한 오누이가 살고 있었어요.  오빠는 너무도 가난하여 양식을 구할 길이 없어 아들아이를 가지길 간절히 바랬고, 자카따룹은 세상을 떠나고 말았어요. 그때 우연히 엄마 나나가 할머니에게 말하는 것을 얻었지요. “할머니, 저하고 결혼하세요.” 사랑하는 아빠를 섬기다 가버린 애가 하루는 꿈에 삼았던 그 새가 살아 있는 걸 보았답니다. 그녀가 창가로 달려가 정말 어느 새 큰 원 안에 갇혀 있게 되었는지 궁금해졌거든요.  그러자 왕비가 이야기를 시작했습니다. “이제 떠돌아다니는 삶에도 지쳤소. 그러니 내가 하느님께 나아갈 수 있도록 도와다오.” 공주가 대답하였어.  “아니,”라며 왕이 자기와 함께 동행한 아기 곰을 데려오셨는데, 그도 마찬가지로 구혼자들이 나타나 이렇게 말하며 그가 원하는 방향으로 헤엄쳐 갔다는 거예요, 그래서 그는 마침내 그를 엄마와 아이를 살려줄 용으로 변신시키고 왕비와 나누어 달라 청하리라 부르게 된 거지 뭐죠. 하지만 이 일로 왕비는 납북절한테는 어떤 약속 말고 그냥 묻어주시기만 하면 된다는 뜻이었습니다.  넌 분명 아름답고 영리한테서 그리고 아무 것도 얻을 수가 있는데 말이 있어 그리할 것이며, 그렇다면 그렇게 말하고 싶진 않으냐 그러면서\n"
     ]
    }
   ],
   "source": [
    "#existing code\n",
    "prompt = \"\"\"\n",
    "옛날 옛적에 한 오누이가 살고 있었어요.\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n",
    "  gen_tokens = finetuned_model.generate(tokens, \n",
    "                                        early_stopping=True,\n",
    "                                        do_sample=True,\n",
    "                                        temperature=0.8, \n",
    "                                        max_length=250, \n",
    "                                        repetition_penalty=3.0,\n",
    "                                        pad_token_id=tokenizer.pad_token_id,\n",
    "                                        eos_token_id=tokenizer.eos_token_id,\n",
    "                                        bos_token_id=tokenizer.bos_token_id,\n",
    "                                        unk_token_id=tokenizer.unk_token_id,)\n",
    "  generated = tokenizer.decode(gen_tokens[0], skip_special_tokens=True).replace(\"\\n\", \" \")\n",
    "  \n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
