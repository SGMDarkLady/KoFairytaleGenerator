{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e5f592ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eunjeon import Mecab\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1698a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a365b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(text):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        text (str): raw text to split to sentences on end of sentences marks.\n",
    "    Returns:\n",
    "        List of sentences from text.\n",
    "    \"\"\"\n",
    "    # Split on end of sentence, but keep the punctuation marks.\n",
    "    text = text.replace('\\n', '')\n",
    "    sentences = text.split('.')\n",
    "    # If the last sentence is ''\n",
    "    if len(sentences) > 1 and len(sentences[-1]) < 3:\n",
    "        sentences.pop()\n",
    "    return sentences\n",
    "\n",
    "def split_words(text):\n",
    "    return re.split(r'[ ](?=[\\w])', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00d38794",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = '''\n",
    "옛날 옛적에 한 오누이가 살고 있었습니다. 한 어머니는 빨갛고 예쁜 옷에 낡았으면 팔려고 했어요. 하지만 손에 꼭 맞는 옷을 만들지 못했해서 결국 엄마는 빨간 헝겊 저고리를 쓰게 되었죠. 어느날 오빠는 새어머니를 모시고 숲으로 갔답니다. 도깨비 할아버지가 방바닥에 앉아 이삭을 하고 있는데 까마귀들이 입을 크게 벌리고 쏟아져 나릅니다 어머나 사람들은 깜짝 놀랐지 뭐예요? 그때 여우가 물었어요. 왜 그래! 옛날 하늘나라 임금님이 눈치채도록 까만 손수레를 밀고 가버렸기에여...임금님은 그 광경을 가만히 지켜보고 있었던 거야. 아니, 저렇게 납작하게 입어봐라 거짓말처럼 큰소리로 말했거든요. 그 모습을 본 임금은 얼굴을 찌푸리며 그만 울상이 되고 말아졌어, 얼굴에는 웃음이 암까지 나왔지만 간신히 몸을 다 펴보았을 때 머리 위에 하얀 구름이 덩쿨이 서고, 앞은 흰 구름 속으로 사라져 버렸다고 하죠. 오빠와 엄마 이야기는 이렇게 이야기를 들으면서 그동안 궁전으로 돌아온 이야기와 앞으로 나갈 일을 이야기 하였답니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f3505c",
   "metadata": {},
   "source": [
    "# 통일성 coherency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28e91014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coherency(text, embedder):\n",
    "    texts_sentences = split_to_sentences(text)\n",
    "    transformed_sentences = embedder.fit_transform(texts_sentences)\n",
    "    similarity = cosine_similarity(transformed_sentences)\n",
    "    return sum(similarity[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d3713a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f8b67f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11463666872271021"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_coherency(ex_text, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8db810",
   "metadata": {},
   "source": [
    "# Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15da7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _readabilty(text):\n",
    "    \"\"\"\n",
    "    Uses length of sentences and length of words.\n",
    "    Higher is for more advanced readers.\n",
    "    If text is sparse i.e. mostly new lines, and doesn't end with an eos -> add a negative cost.  \n",
    "    Args:\n",
    "        text (str): original text to return score for.\n",
    "        texts_sentences (list): text split to sentences. \n",
    "    \"\"\"\n",
    "    texts_sentences = split_to_sentences(text)\n",
    "    txt_words = split_words(text)\n",
    "    num_letters = sum(len(word) for word in txt_words)\n",
    "    num_words = len(txt_words)\n",
    "    num_sent = len(texts_sentences)\n",
    "\n",
    "    # check if a \"sparse\" sentence\n",
    "    if num_sent == 1:\n",
    "        new_line_threshold = 0 if num_words == 0 else num_words // 4\n",
    "        if texts_sentences[0].count('\\n') > new_line_threshold or not re.search(r'(?<![A-Z])[.!?;\"]+', texts_sentences[0]):\n",
    "            num_sent = 0\n",
    "\n",
    "    letters_per_word = -10 if num_words == 0 else num_letters/num_words\n",
    "    words_per_sentence = -10 if num_sent == 0 else num_words/num_sent\n",
    "    # 0.5 to weight words_per_sentence higher\n",
    "    return 0.5*letters_per_word + words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a0a6969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.972666666666665"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _readabilty(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd92d1",
   "metadata": {},
   "source": [
    "# Diversity\n",
    "- stopwords에 들어간 표현을 제외하고, 문장에서 얼마나 다양한 표현을 써냈는지 평가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0167c405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = text.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    tokenizer = Mecab('C:\\mecab\\mecabrc').morphs\n",
    "    tokens = tokenizer(text)\n",
    "    stopwords = ['은','는','이','가']\n",
    "    \n",
    "    text = [word for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0860beeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _diversity(text):\n",
    "    \"\"\"\n",
    "    Fraction of unique words from the total number of words (exclusing stop words).\n",
    "    Higher is more diversified.\n",
    "    Args:\n",
    "        filtered_words (list): set of non-stop tokenized words. \n",
    "        filtered_words_set (set): unique filtered words.\n",
    "    \"\"\"\n",
    "    filtered_words = remove_stopwords(text)\n",
    "    filtered_words_set = set(filtered_words)\n",
    "    \n",
    "    # If empty sentence or only white space or \\n or too repetitive.\n",
    "    if len(filtered_words_set) < 5:\n",
    "        return 0\n",
    "\n",
    "    return len(filtered_words_set) / len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7ee795f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6608695652173913"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_diversity(ex_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf199b",
   "metadata": {},
   "source": [
    "# simplicity\n",
    "- fine tuned 모델에서 named entity를 제외한 most freq words 정의 필요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "918a90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simplicity(text):\n",
    "    \"\"\"\n",
    "    Fraction of most frequent words from generated text.\n",
    "    Args:\n",
    "        filtered_words_set (set): set of non-stop, non-punctuation words. \n",
    "    \"\"\"\n",
    "    filtered_words = remove_stopwords(text)\n",
    "    filtered_words_set = set(filtered_words)\n",
    "    \n",
    "    return len(filtered_words_set.intersection(constants.SEVEN_PREC_MOST_FREQ_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4be485c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'constants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3244\\3308967024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0m_simplicity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mex_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3244\\1551805251.py\u001b[0m in \u001b[0;36m_simplicity\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mfiltered_words_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_words_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconstants\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSEVEN_PREC_MOST_FREQ_WORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'constants' is not defined"
     ]
    }
   ],
   "source": [
    "_simplicity(ex_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8d165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
